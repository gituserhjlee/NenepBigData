{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "DatasetPreprocessing.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thdgmltjd123/NenepBigData/blob/master/preprocessing/DatasetPreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5C2huJlo__J",
        "outputId": "deae2065-08d2-4cdd-fb54-d1ac448d58f4"
      },
      "source": [
        "#구글 드라이브와 코랩 마운트하기\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyiAmoc-o4S5"
      },
      "source": [
        "%matplotlib inline\n",
        "import copy\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import emoji\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from gensim.models.word2vec import Word2Vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3lonPkYo4TF"
      },
      "source": [
        "label = os.listdir(\"testDatas\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ev-eCwkSo4TG",
        "outputId": "4bab4fbb-bb80-4c4f-c1fe-c88c6dbf2244"
      },
      "source": [
        "label"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['actionadventure',\n",
              " 'animation',\n",
              " 'biography',\n",
              " 'crimeaction',\n",
              " 'horror',\n",
              " 'mysterythriller',\n",
              " 'romanticcomedy',\n",
              " 'scifi',\n",
              " 'war']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "9tOc37wro4TJ"
      },
      "source": [
        "labelled_data = []\n",
        "la = labelled_data.append\n",
        "for gidx, genre in enumerate(label):\n",
        "    la(labeling_data(os.listdir(\"testDatas\\\\\"+genre),\"testDatas\\\\\"+genre+\"\\\\\", gidx))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQIVWRT8o4TK"
      },
      "source": [
        "genre_df = []\n",
        "for i in range(9):\n",
        "    genre_df.append(labelled_data[i][0].append(labelled_data[i][1:]))\n",
        "\n",
        "dataset = genre_df[0].append(genre_df[1:],sort = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0rmIkhyo4TL",
        "outputId": "b3439842-5ce1-41dc-dddf-5520e99931a3"
      },
      "source": [
        "dataset = dataset.sample(frac = 1).reset_index(drop = True)\n",
        "dataset = dataset.dropna().reset_index(drop = True)\n",
        "len(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "146879"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6B6ensuRo4TM"
      },
      "source": [
        "def labeling_data(folder_list, path, label):\n",
        "    df_list = []\n",
        "    for movie in folder_list:\n",
        "        data = pd.read_csv(path+movie, names = [\"idx\", \"review\"])\n",
        "        data[\"label\"] = label\n",
        "        data = data.iloc[1:,1:]\n",
        "        df_list.append(data)\n",
        "    return df_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6m4ZLH1io4TM"
      },
      "source": [
        "path = os.listdir(\"reviewDatas\")\n",
        "def load_data(folder_list, path):\n",
        "    movie_review = []\n",
        "    ma = movie_review.append\n",
        "    for movie in folder_list:\n",
        "        data = pd.read_csv(path+movie, names = [\"idx\", \"review\"])\n",
        "        data = data.iloc[1:,1:]\n",
        "        review_list = []\n",
        "        ra = review_list.append\n",
        "        for i in range(len(data)):\n",
        "            ra(data.iloc[i,0])\n",
        "        ma(review_list)\n",
        "    return movie_review"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TO83QoY1o4TN"
      },
      "source": [
        "def preprocessing(df):\n",
        "    review_list = list(df[\"review\"])\n",
        "    multi_emoji = re.compile(r'[:]{2,}[\\w_-]+')\n",
        "    shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
        "    punc = re.compile(r'[\\W_]+')\n",
        "    first_upper = re.compile(r'\\s*[A-Z][a-z0-9]+')\n",
        "    digit = re.compile(r'\\d+')\n",
        "\n",
        "    for i in tqdm(range(len(review_list))):\n",
        "        try:\n",
        "            review_list[i] = multi_emoji.sub('',emoji.demojize(review_list[i]))\n",
        "            review_list[i] = shortword.sub('',review_list[i])\n",
        "            review_list[i] = punc.sub(' ',review_list[i])\n",
        "            review_list[i] = first_upper.sub('',review_list[i])\n",
        "            review_list[i] = digit.sub('',review_list[i])\n",
        "            review_list[i] = shortword.sub('',review_list[i])\n",
        "            review_list[i] = review_list[i].lower()\n",
        "            review_list[i] = review_list[i].strip()\n",
        "        except:\n",
        "            print(i)\n",
        "\n",
        "        df[\"review\"] = review_list\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "1xpaFLmvo4TP",
        "outputId": "28a638b0-db17-41f1-ae4f-7dee45a550bd"
      },
      "source": [
        "df = preprocessing(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 146879/146879 [1:51:10<00:00, 22.02it/s] \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIcqai53o4TP"
      },
      "source": [
        "df.to_csv(\"preprocessing.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvZCxPQAo4TQ"
      },
      "source": [
        "df = pd.read_csv(\"preprocessing.csv\", index_col = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rexsuqZ1o4TR"
      },
      "source": [
        "#불용어 목록 만들기\n",
        "def make_stopwords() :\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    google_sw = \"a about above after again against all am an and any are aren't as at be because been before being below between both but by can't cannot could couldn't did didn't do does doesn't doing don't down during each few for from further had hadn't has hasn't have haven't having he he'd he'll he's her here here's hers herself him himself his how how's i i'd i'll i'm i've if in into is isn't it it's its itself let's me more most mustn't my myself no nor not of off on once only or other ought our ours ourselves out over own same shan't she she'd she'll she's should shouldn't so some such than that that's the their theirs them themselves then there there's these they they'd they'll they're they've this those through to too under until up very was wasn't we we'd we'll we're we've were weren't what what's when when's where where's which while who who's whom why why's with won't would wouldn't you you'd you'll you're you've your yours yourself yourselves\"\n",
        "    google_sw = set(google_sw.split(' '))\n",
        "    no_upper_sw = set(\"arent cant couldnt didnt doesnt dont hadnt hasnt hed hes heres hows id ill im ive isnt lets mustnt shant shed shes shouldnt thats theres theyd theyll theyre theyve wasnt wed weve werent whats whens wheres whos whys wont wouldnt youd youll youre youve\".split(' '))\n",
        "\n",
        "    stop_words = stop_words | google_sw | no_upper_sw\n",
        "    return stop_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "f6jV5KnTo4TS"
      },
      "source": [
        "def elim_stopwords(df):\n",
        "    stop_words = make_stopwords()\n",
        "    review_list = list(df[\"review\"])\n",
        "    for idx, review in enumerate(review_list):\n",
        "        li = re.split(\"\\s+\",review)\n",
        "        li = [item for item in li if item not in stop_words]\n",
        "        review_list[idx] = \" \".join(li)\n",
        "        \n",
        "    df[\"review\"] = review_list\n",
        "    print(\"eliminating stopwords is done --> pos tagging\")\n",
        "    \n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "pgeM6R_eo4TT",
        "outputId": "c36b9e1e-4511-4290-ebdc-1e74d9c16a6c"
      },
      "source": [
        "df=df.dropna().reset_index(drop=False)\n",
        "df = elim_stopwords(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eliminating stopwords is done --> pos tagging\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfWRA2ojo4TU",
        "outputId": "46561009-a95c-4dbe-c5ea-17be706baf33"
      },
      "source": [
        "len(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "146718"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9B_aEC0o4TV"
      },
      "source": [
        "def get_wordnet_pos(treebank_tag):\n",
        "\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return ''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lzg1uxBko4TV"
      },
      "source": [
        "def tagging_lemmatize(df, min_words = 5):\n",
        "    n=WordNetLemmatizer()\n",
        "    review_list = list(df[\"review\"])\n",
        "\n",
        "    for idx, review in enumerate(tqdm(review_list)):\n",
        "        li = review.split(\" \")\n",
        "        if len(li) > min_words:\n",
        "            li = pos_tag(li)\n",
        "            for k, token in enumerate(li):\n",
        "                try:\n",
        "                    li[k] = n.lemmatize(token[0],get_wordnet_pos(token[1]))\n",
        "                except KeyError:\n",
        "                    li[k] = n.lemmatize(token[0])\n",
        "        review_list[idx] = \" \".join(li)\n",
        "    df[\"review\"] = review_list\n",
        "    elim_idx = df[df[\"review\"]==''].index\n",
        "    df = df.drop(elim_idx)\n",
        "    \n",
        "    return df.reset_index(drop = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "KHgL4b5Oo4TW",
        "outputId": "bea105fe-9885-46cc-cf06-8ab0b713d6d8"
      },
      "source": [
        "df = tagging_lemmatize(df)\n",
        "df = df.iloc[:,1:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 146718/146718 [27:00<00:00, 90.51it/s] \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0MkUhnFo4TX"
      },
      "source": [
        "df.to_csv(\"lemmatizing.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhS3BmdDo4TX"
      },
      "source": [
        "review_train, review_test, label_train, label_test = train_test_split(dataset.iloc[:,0],dataset.iloc[:,1], test_size = 0.3, stratify = dataset.iloc[:,1], random_state = 777)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "qsmg_FPuo4TY",
        "outputId": "7b9fbc14-8f17-4569-e91e-9513bf41bfd8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5    7674\n",
              "3    7308\n",
              "7    6813\n",
              "0    5215\n",
              "2    4801\n",
              "8    4541\n",
              "1    3396\n",
              "4    2869\n",
              "6    1447\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQTQGKkto4TY"
      },
      "source": [
        "## 전처리 방안\n",
        "\n",
        "+ 이모티콘을 단어로 변환 후 중복된 이모티콘을 1개로 만들기   \n",
        "+ 단어의 길이가 1에서 2인 것 제거    \n",
        "+ 리뷰의 구두점들을 제거      \n",
        "+ 첫번째 단어가 대문자거나 대문자로 되어있는 모든 단어들 제거   \n",
        "+ 숫자 제거      \n",
        "+ 제거되지 않은 대문자 모임 소문자화 후 양끝 공백제거"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qf0JJFj4o4TZ",
        "outputId": "03b3168b-3233-407a-c14e-4803365cac8f"
      },
      "source": [
        "all_token = []\n",
        "aa = all_token.append\n",
        "for idx,review_list in enumerate(movie_review):\n",
        "    token_list = preprocessing(review_list)\n",
        "    aa(token_list)\n",
        "    if (idx+1) % 3 == 0:\n",
        "        print(\"{}%\".format((idx+1) / len(movie_review) * 100), end = \" \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10.0% 20.0% 30.0% 40.0% 50.0% 60.0% 70.0% 80.0% 90.0% 100.0% "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9n-IX9qRo4Ta"
      },
      "source": [
        "all_token 리스트의 길이 30개(영화 수)    \n",
        "all_token\\[0\\]는 첫번째 영화의 token_list  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vjkAczzo4Tb"
      },
      "source": [
        "## 불용어 처리\n",
        "\n",
        "추가사항   \n",
        "구글의 stopwords english와 합집합.   \n",
        "리뷰에 의도적으로 어퍼스토로피를 빼고 작성한 경우 존재.   \n",
        "stopwords가 작동을 하지 않음.    \n",
        "따라서 적당히 단어가 되지 않는 선에서 stopwords 추가   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUsw5FYqo4Tb"
      },
      "source": [
        "all_token = elim_stopwords(all_token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfU0DOwoo4Tc"
      },
      "source": [
        "# 제거해야 하는 리뷰들    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cPXWAJWo4Tc"
      },
      "source": [
        "+ 첫번째 글자를 항상 대문자로 작성하였음...     \n",
        "    \n",
        "데드풀 938,\"Deadpool, Was The Perfect Comic Book Movie, Ryan Reynolds Was Born To Play The Character, And The Movie Is A Very Good Different Kind Of Movie, It's Fun To Watch! Has Great Jokes It's Fun! Funny! Has Great Wallbreakers! Good Amount Of Action! You Don't Want The Movie To End It's Been A While Sense I Seen A Superhero That's This Good And Perfect! Ryan Reynolds Is Inspiring In This Movie Makes You Wanna Become An Actor So You Can Play A Character Like This! This Movie Deserves To Be On Imdbs Top Movies, It Can't Get Better Than That, Hoping To See More Deadpool In The Future, One Of The Best Superhero/Antiheros Ever! Movie Had A Simple But Fun Story!\"    \n",
        "    \n",
        "기생충 759,\"I'm An Indian.I Watched It After A Lot Of Recommendation,But It Never Fullfilled My Expectations.The Movie Shows The Division In Between Peoples Such As High Class And Low Class,It Presents In The Way Of A Comedy Drama.I'm Not Satisfied On The Movie Maybe Because Of I'd Watch Lot Of Indian Movies Which Has These Kind Of Story.\"     \n",
        "    \n",
        "Ludo 67 \"A Nice Movie Which Has Everything In It. If Creators Cut Duration By 30 Minutes Then It's Became More Enjoyable.\"    \n",
        "     \n",
        "Mulan 1337,\"I Dislike When People Compare The Mulan Live Action Movie To The Original Animated Movie There Two Totally Different   Movies.I Thought The Cinematography Was Absolutely Astounding,The Characters Where Amazing And The Special Effects Where Great.\"   \n",
        "    \n",
        "SooraraiPottru 287 \"Good Direction Fabulous Acting Good Cinematography\"    \n",
        "    \n",
        "스파이더맨 146,\"Animation 10/10     \n",
        "Story 10/10     \n",
        "Direction 10/10     \n",
        "Script 10/10     \n",
        "Imagination 100/10Loved it....\"     \n",
        "\n",
        "TheTrialoftheChicago7 25, \"Performances: 10/10 Pacing: 7/10 Plot: 9/10 Writing: 10/10 Cinematography: 7/10 Soundtrack: 7/10        Enjoyability: 8/10\"         \n",
        "    \n",
        "나크나로크 907,Niiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiicccccccccccccccccccccccccccceeeeeeeeeeeeeeeeeeeeeeeeeeee     "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqYXMbsko4Th"
      },
      "source": [
        "# Pos-tagging    \n",
        "\n",
        "품사 태깅"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "JVDxXZwFo4Ti",
        "outputId": "f8c3cd35-331a-4b0f-d0b0-5dd4fbc4b5d7"
      },
      "source": [
        "all_token = pos_tagging(all_token)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "progress = 10.0%\n",
            "progress = 20.0%\n",
            "progress = 30.0%\n",
            "progress = 40.0%\n",
            "progress = 50.0%\n",
            "progress = 60.0%\n",
            "progress = 70.0%\n",
            "progress = 80.0%\n",
            "progress = 90.0%\n",
            "progress = 100.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snDDvAa5o4Tj"
      },
      "source": [
        "# Stemming and Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOeNpyLCo4Tj"
      },
      "source": [
        "감성단어는 보통 형용사가 많기 때문에    \n",
        "Stemming은 과한 일반화가 될 수 있으므로 시행하지 않음   \n",
        "pos-tagging(품사 태깅) 후 Lemmatization만 시행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pqg0LhhQo4Tj",
        "outputId": "7d96223d-3280-4464-8785-e68e5e6ce408"
      },
      "source": [
        "all_token[8][759]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('never', 'RB'),\n",
              " ('seen', 'VBN'),\n",
              " ('movie', 'NN'),\n",
              " ('every', 'DT'),\n",
              " ('little', 'JJ'),\n",
              " ('detail', 'NN'),\n",
              " ('dialogue', 'NN'),\n",
              " ('object', 'NN'),\n",
              " ('foreshadows', 'VBZ'),\n",
              " ('things', 'NNS'),\n",
              " ('come', 'VBP'),\n",
              " ('fabulous', 'JJ'),\n",
              " ('way', 'NN'),\n",
              " ('made', 'VBD'),\n",
              " ('film', 'NN'),\n",
              " ('entertaining', 'VBG'),\n",
              " ('lots', 'NNS'),\n",
              " ('suspense', 'JJ'),\n",
              " ('end', 'NN')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "grMZjBhQo4Tk",
        "outputId": "48bfcf4e-4af5-421b-d878-5debe2162354"
      },
      "source": [
        "all_token = lemmatization(all_token)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "progress = 10.0%\n",
            "progress = 20.0%\n",
            "progress = 30.0%\n",
            "progress = 40.0%\n",
            "progress = 50.0%\n",
            "progress = 60.0%\n",
            "progress = 70.0%\n",
            "progress = 80.0%\n",
            "progress = 90.0%\n",
            "progress = 100.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CxgwcvCo4Tk"
      },
      "source": [
        "all_token\\[0\\~29\\] --> 영화 0번부터 29번까지    \n",
        "all_token\\[0\\]\\[0\\~k\\] --> 0번 영화의 0~k번째 리뷰의 토큰 리스트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnnG21bjo4Tk"
      },
      "source": [
        "path = os.listdir(r\"testDatas\\actionadventure\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ58RAk_o4Tl",
        "outputId": "7f79156b-8d0c-4a49-b5ab-e9f83ad6d2fe"
      },
      "source": [
        "path"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Apocalypto.csv',\n",
              " 'AvengersEndgame.csv',\n",
              " 'EnolaHolmes.csv',\n",
              " 'Gladiator.csv',\n",
              " 'IntheHeartoftheSea.csv',\n",
              " 'KingArthur.csv',\n",
              " 'KingdomofHeaven.csv',\n",
              " 'KingKong.csv',\n",
              " 'RobinHoodPrinceofThieves.csv',\n",
              " 'Shichininnosamurai.csv',\n",
              " 'SnowWhiteandtheHuntsman.csv',\n",
              " 'Superman.csv',\n",
              " 'ThePoseidonAdventure.csv',\n",
              " 'TheRevenant.csv',\n",
              " 'Wilow.csv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    }
  ]
}